{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow Webscraping\n",
    "## Project description\n",
    "- the goal the project\n",
    "- pricing, sqft, lot size, etc.\n",
    "- the whole bay area: counties\n",
    "## Explore the dataset\n",
    "-goals exploring dataset\n",
    "\n",
    "## Price prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create seperate file to read into the url\n",
    "\n",
    "- Define  list_or_url:\n",
    "    clean up r, p, t\n",
    "    make it readable\n",
    "    \n",
    "Move import statement outside as a global\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "\n",
    "import fake_useragent\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling URL from individual property card on Zillow\n",
    "Keep in mind that html/json script is always changing. User will have to check if scripts are up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method Fetch Print: <Response [200]>\n",
      "Method Fetch Print: <Response [200]>\n",
      "Method Fetch Print: <Response [200]>\n",
      "Method Fetch Print: <Response [200]>\n",
      "Method Fetch Print: <Response [200]>\n",
      "Method Fetch Print: <Response [200]>\n",
      "Method Fetch Print: <Response [200]>\n",
      "Method Fetch Print: <Response [200]>\n",
      "Method Fetch Print: <Response [200]>\n"
     ]
    }
   ],
   "source": [
    "class ZillowScraper():\n",
    "    \"\"\"\n",
    "    This class here will use the header to by pass the zillow security features and parse through each of the\n",
    "    property card and pull the URL from each of the cards and then save it to a csv file.\n",
    "    ***Important***\n",
    "    User will have to manually adjust the page range due to listing constantly being updated.\n",
    "    Recommend looking at page first and then adjusting the range().\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    headers = {\n",
    "        \n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'accept-encoding': 'gzip, deflate, br',\n",
    "        'accept-language': 'en-US,en;q=0.9',\n",
    "        'cache-control': 'max-age=0',\n",
    "        'cookie': 'zguid=23|%248ce812ae-6c06-4282-a185-aa150258dbf1; zgsession=1|8334c6a7-229b-4e76-848b-82389e70947e; _ga=GA1.2.1328246037.1599671175; _gid=GA1.2.1914266924.1599671175; zjs_user_id=null; zjs_anonymous_id=%228ce812ae-6c06-4282-a185-aa150258dbf1%22; JSESSIONID=1708916C4FBDF470F11ECAC8FF6B9FE4; _gcl_au=1.1.1164169983.1599671175; KruxPixel=true; DoubleClickSession=true; _pxvid=c4cdb03c-f2be-11ea-8c78-0242ac120009; _fbp=fb.1.1599671175499.1034465933; _pin_unauth=dWlkPVpXVXpabUpsT1dZdE1URXlOeTAwTlRObExXSmpNbUV0TW1VM05qTTVPR0psWVRNeiZycD1abUZzYzJV; KruxAddition=true; __gads=ID=e626110456c836d9:T=1599671193:S=ALNI_Ma54r9zXdXhEGsTRKIfa-inIvF-fw; _uetsid=3f2d45905ff726b6499b8706116c0114; _uetvid=3559190482ab0e9b5caa1cac51f7c1a0; AWSALB=dMrLy2l7uELP5EH+AxX5MMqzhulxSiIOT4rBks/Jw84brvFEjZkNj5UbCeNRDfa3IFzxyG6ZOLgMyOhzt01WdfX2c7Vq0fTna+QWhxXFGphrS3FpESujWPIh9iuZ; AWSALBCORS=dMrLy2l7uELP5EH+AxX5MMqzhulxSiIOT4rBks/Jw84brvFEjZkNj5UbCeNRDfa3IFzxyG6ZOLgMyOhzt01WdfX2c7Vq0fTna+QWhxXFGphrS3FpESujWPIh9iuZ; search=6|1602264036452%7Crect%3D37.76954206618076%252C-121.40946584570312%252C36.84547153257956%252C-122.34055715429687%26rid%3D33839%26disp%3Dmap%26mdm%3Dauto%26p%3D1%26z%3D0%26type%3Dhouse%26pt%3Dpmf%252Cpf%26fs%3D1%26fr%3D0%26mmm%3D1%26rs%3D0%26ah%3D0%26singlestory%3D0%26housing-connector%3D0%26abo%3D0%26garage%3D0%26pool%3D0%26ac%3D0%26waterfront%3D0%26finished%3D0%26unfinished%3D0%26cityview%3D0%26mountainview%3D0%26parkview%3D0%26waterview%3D0%26hoadata%3D1%26zillow-owned%3D0%263dhome%3D0%09%0933839%09%09%09%09%09%09; _px3=7b4431112be2b28f4f7fa94f9d1a8de95193400e4c165b534dae141a82fac7d2:SYa7vqmqqOxn149LrL2d2vYCEiZZc6tT5Wdzvc2reryEePknRoBraFZnhzzHAgcxK+HcSZdbDYdBM1yOJvSa9g==:1000:gjXwxqSPFrWbs8JpZxS60goxu4ObF8+WiWxJdxryvNkuXmMvRANUuN8owbmRu3XZvQgZgh28PHhPvoR9gbdsln7BBiLMPgIW0LD+sdtC06kttkL/eEFdNP69vgnKHxokeyInMeXb0AyYzbBaHshBbN/g7SwnMXJa+7jm7hRZSgA=',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36'\n",
    "\n",
    "    }\n",
    "    \n",
    "    def fetch(self, URL, search_parameters):\n",
    "      #Print request code\n",
    "        web_page = requests.get(URL, headers = self.headers, params = search_parameters)\n",
    "        print(\"Method Fetch Print:\",web_page)\n",
    "\n",
    "        return web_page\n",
    "    \n",
    "    def parse(self, web_page):\n",
    "      #Turning webpage to soup and reading it as lxml\n",
    "        content = BS(web_page, 'lxml')\n",
    "        pages = content.find('ul',{'class': 'photo-cards photo-cards_wow photo-cards_short photo-cards_extra-attribution'})\n",
    "      #For each card on Zillow site we are pulling the url of each one.\n",
    "        for property in pages.contents:\n",
    "            script = property.find('script', {'type': 'application/ld+json'})\n",
    "            if script:\n",
    "                script_json = json.loads(script.contents[0])\n",
    "                self.output.append({\n",
    "                    'url': script_json['url']         \n",
    "                })\n",
    "\n",
    "    def to_csv(self):\n",
    "      #Creating a csv file\n",
    "        with open('/Users/truon/Documents/BAN612/Zillow/GitTestRun/ZillowForSaleURLFeb12.csv', 'w') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, lineterminator='\\n', fieldnames=self.output[0].keys())\n",
    "            writer.writeheader()\n",
    "            for row in self.output:\n",
    "                writer.writerow(row)    \n",
    "    \n",
    "    def run(self):\n",
    "      #Main page of zillow with the city.\n",
    "        URL = 'https://www.zillow.com/homes/San-Jose,-CA_rb/'\n",
    "      #Pages on zillow that we will go through the range will have to be manually adjusted due to listing being updated.\n",
    "      #If there are 16 pages of property card count ends at 15.\n",
    "        for page in range(1,10):\n",
    "            search_parameters = {\n",
    "                'searchQueryState': '{\"pagination\":{\"currentPage\": %s},\"mapBounds\":{\"west\":-122.34055715429687,\"east\":-121.40946584570312,\"south\":36.84547153257956,\"north\":37.76954206618076},\"regionSelection\":[{\"regionId\":33839,\"regionType\":6}],\"isMapVisible\":True,\"filterState\":{\"sort\":{\"value\":\"globalrelevanceex\"},\"con\":{\"value\":False},\"mf\":{\"value\":False},\"manu\":{\"value\":False},\"land\":{\"value\":False},\"tow\":{\"value\":False},\"apa\":{\"value\":False}},\"isListVisible\":True}' %page\n",
    "            }    \n",
    "            res = self.fetch(URL,search_parameters)\n",
    "            self.parse(res.text)\n",
    "            time.sleep(1)\n",
    "        self.to_csv()\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scraper = ZillowScraper()\n",
    "    scraper.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually entering Proxies\n",
    "    - Avoid Captcha and bans from zillow website.\n",
    "    - Allow users to change their IP, socks, and proxy.\n",
    "### How it works:\n",
    "When driver variable is called it will open up the chromedriver extension. Fake useragent will then enter in the proxy, socks, and ssl the user agent has manually entered in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-abba95e2793f>:12: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(chrome_options=options, executable_path=r'/Users/truon/Documents/BAN612/chromedriver', desired_capabilities=capabilities)\n"
     ]
    }
   ],
   "source": [
    "prox = Proxy()\n",
    "prox.proxy_type = ProxyType.MANUAL\n",
    "prox.http_proxy = \"85.175.227.3:7018\" #Change this ip code if needed\n",
    "#prox.socks_proxy = \"ip_addr:port\"\n",
    "#prox.ssl_proxy = \"ip_addr:port\"\n",
    "capabilities = webdriver.DesiredCapabilities.CHROME\n",
    "prox.add_to_capabilities(capabilities)\n",
    "options = Options()\n",
    "ua = UserAgent()\n",
    "userAgent = ua.random\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "driver = webdriver.Chrome(chrome_options=options, executable_path=r'/Users/truon/Documents/BAN612/chromedriver', desired_capabilities=capabilities)\n",
    "#https://chromedriver.chromium.org/downloads\n",
    "#Make sure you have the right Chrome Version on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-aff2aaf8ccdd>:234: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Status\"]=scrape_data[0]\n",
      "<ipython-input-26-aff2aaf8ccdd>:235: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Num of bed\"]=scrape_data[1]\n",
      "<ipython-input-26-aff2aaf8ccdd>:236: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Num of bath\"]=scrape_data[2]\n",
      "<ipython-input-26-aff2aaf8ccdd>:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Sqft\"]=scrape_data[3]\n",
      "<ipython-input-26-aff2aaf8ccdd>:238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Listing Price\"]=scrape_data[4]\n",
      "<ipython-input-26-aff2aaf8ccdd>:239: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"House Type\"]=scrape_data[5]\n",
      "<ipython-input-26-aff2aaf8ccdd>:240: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Year built\"]=scrape_data[6]\n",
      "<ipython-input-26-aff2aaf8ccdd>:241: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Heating\"]=scrape_data[7]\n",
      "<ipython-input-26-aff2aaf8ccdd>:242: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Cooling\"]=scrape_data[8]\n",
      "<ipython-input-26-aff2aaf8ccdd>:243: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Parking Type\"]=scrape_data[9]\n",
      "<ipython-input-26-aff2aaf8ccdd>:244: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Num of parking space\"]=scrape_data[10]\n",
      "<ipython-input-26-aff2aaf8ccdd>:245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"HOA\"]=scrape_data[11]\n",
      "<ipython-input-26-aff2aaf8ccdd>:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Price per sqft\"]=scrape_data[12]\n",
      "<ipython-input-26-aff2aaf8ccdd>:247: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Lot Size in sqft\"]=scrape_data[13]\n",
      "<ipython-input-26-aff2aaf8ccdd>:248: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Days On Zillow\"]=scrape_data[14]\n",
      "<ipython-input-26-aff2aaf8ccdd>:249: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Property Url\"]=scrape_data[15]\n",
      "<ipython-input-26-aff2aaf8ccdd>:250: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Street\"]=scrape_data[16]\n",
      "<ipython-input-26-aff2aaf8ccdd>:251: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"City\"]=scrape_data[17]\n",
      "<ipython-input-26-aff2aaf8ccdd>:252: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"State\"]=scrape_data[18]\n",
      "<ipython-input-26-aff2aaf8ccdd>:253: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Zip Code\"]=scrape_data[19]\n",
      "<ipython-input-26-aff2aaf8ccdd>:254: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_output[\"Full Address\"]=scrape_data[20]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>Status</th>\n",
       "      <th>Num of bed</th>\n",
       "      <th>Num of bath</th>\n",
       "      <th>Sqft</th>\n",
       "      <th>Listing Price</th>\n",
       "      <th>House Type</th>\n",
       "      <th>Year built</th>\n",
       "      <th>Heating</th>\n",
       "      <th>Cooling</th>\n",
       "      <th>...</th>\n",
       "      <th>HOA</th>\n",
       "      <th>Price per sqft</th>\n",
       "      <th>Lot Size in sqft</th>\n",
       "      <th>Days On Zillow</th>\n",
       "      <th>Property Url</th>\n",
       "      <th>Street</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Full Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.zillow.com/homedetails/0-Cherrysto...</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3400</td>\n",
       "      <td>900000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Time on Zillow189 days</td>\n",
       "      <td>https://www.zillow.com/homedetails/0-Cherrysto...</td>\n",
       "      <td>0 Cherrystone Dr</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95128</td>\n",
       "      <td>0 Cherrystone Dr,San Jose, CA 95128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.zillow.com/homedetails/1715-Hogar-...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1288</td>\n",
       "      <td>998999</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Time on Zillow184 days</td>\n",
       "      <td>https://www.zillow.com/homedetails/1715-Hogar-...</td>\n",
       "      <td>1715 Hogar Dr</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95124</td>\n",
       "      <td>1715 Hogar Dr,San Jose, CA 95124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.zillow.com/homedetails/476-Arleta-...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1196</td>\n",
       "      <td>899888</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Time on Zillow6 days</td>\n",
       "      <td>https://www.zillow.com/homedetails/476-Arleta-...</td>\n",
       "      <td>476 Arleta Ave</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95128</td>\n",
       "      <td>476 Arleta Ave,San Jose, CA 95128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.zillow.com/homedetails/393-Springp...</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2075</td>\n",
       "      <td>1098000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Time on Zillow4 days</td>\n",
       "      <td>https://www.zillow.com/homedetails/393-Springp...</td>\n",
       "      <td>393 Springpark Cir</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95136</td>\n",
       "      <td>393 Springpark Cir,San Jose, CA 95136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.zillow.com/homedetails/5266-Vera-L...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1874</td>\n",
       "      <td>799000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Time on Zillow12 days</td>\n",
       "      <td>https://www.zillow.com/homedetails/5266-Vera-L...</td>\n",
       "      <td>5266 Vera Ln</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95111</td>\n",
       "      <td>5266 Vera Ln,San Jose, CA 95111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url Status  Num of bed  \\\n",
       "0  https://www.zillow.com/homedetails/0-Cherrysto...   None           4   \n",
       "1  https://www.zillow.com/homedetails/1715-Hogar-...   None           3   \n",
       "2  https://www.zillow.com/homedetails/476-Arleta-...   None           3   \n",
       "3  https://www.zillow.com/homedetails/393-Springp...   None           4   \n",
       "4  https://www.zillow.com/homedetails/5266-Vera-L...   None           3   \n",
       "\n",
       "   Num of bath  Sqft  Listing Price House Type Year built Heating Cooling  \\\n",
       "0          3.0  3400         900000       None       None    None    None   \n",
       "1          2.0  1288         998999       None       None    None    None   \n",
       "2          2.0  1196         899888       None       None    None    None   \n",
       "3          3.0  2075        1098000       None       None    None    None   \n",
       "4          3.0  1874         799000       None       None    None    None   \n",
       "\n",
       "   ...   HOA  Price per sqft Lot Size in sqft          Days On Zillow  \\\n",
       "0  ...  None            None             None  Time on Zillow189 days   \n",
       "1  ...  None            None             None  Time on Zillow184 days   \n",
       "2  ...  None            None             None    Time on Zillow6 days   \n",
       "3  ...  None            None             None    Time on Zillow4 days   \n",
       "4  ...  None            None             None   Time on Zillow12 days   \n",
       "\n",
       "                                        Property Url              Street  \\\n",
       "0  https://www.zillow.com/homedetails/0-Cherrysto...    0 Cherrystone Dr   \n",
       "1  https://www.zillow.com/homedetails/1715-Hogar-...       1715 Hogar Dr   \n",
       "2  https://www.zillow.com/homedetails/476-Arleta-...      476 Arleta Ave   \n",
       "3  https://www.zillow.com/homedetails/393-Springp...  393 Springpark Cir   \n",
       "4  https://www.zillow.com/homedetails/5266-Vera-L...        5266 Vera Ln   \n",
       "\n",
       "       City State Zip Code                           Full Address  \n",
       "0  San Jose    CA    95128    0 Cherrystone Dr,San Jose, CA 95128  \n",
       "1  San Jose    CA    95124       1715 Hogar Dr,San Jose, CA 95124  \n",
       "2  San Jose    CA    95128      476 Arleta Ave,San Jose, CA 95128  \n",
       "3  San Jose    CA    95136  393 Springpark Cir,San Jose, CA 95136  \n",
       "4  San Jose    CA    95111        5266 Vera Ln,San Jose, CA 95111  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_property_detail(property_url_list, driver, start_index, end_index):\n",
    "    Price_per_sqft = []\n",
    "    status = []\n",
    "    listPrice = []\n",
    "    Type = []\n",
    "    Year = []\n",
    "    Heating = []\n",
    "    Cooling = []\n",
    "    Parking = []\n",
    "    Space =[]\n",
    "    HOA = []\n",
    "    Lot = []\n",
    "    daysOnZillow = []\n",
    "    UrlList = []\n",
    "    fullAddress =[]\n",
    "    streetList = []\n",
    "    cityList = []\n",
    "    stateList =[]\n",
    "    zipCodeList = []\n",
    "    num_bed= []\n",
    "    num_bath= []\n",
    "    sqft_list = []\n",
    "       \n",
    "    driver.get(str(property_url_list[start_index]))\n",
    "    soup = BS(driver.page_source, 'html.parser')  \n",
    "    for i in range(start_index,end_index):\n",
    "        #check status\n",
    "        try:\n",
    "            status_soup=soup.find_all(\"span\", class_=\"qf5kuj-5 bkSUKT ds-status-details\")\n",
    "            if len(status_soup) == 0:\n",
    "                status_soup=soup.find_all(\"span\", class_=\"sc-pscky hEQFyS ds-status-details\")\n",
    "            if 'for sale' in status_soup[0].text.lower():\n",
    "                status.append('For Sale')\n",
    "            else:\n",
    "                status.append(None)\n",
    "        except:\n",
    "            status.append(None)\n",
    "        prop_soup=soup.findAll(\"span\", class_=\"ds-bed-bath-living-area\")\n",
    "        time.sleep(np.random.randint(5,10))\n",
    "    #Bed\n",
    "        try:\n",
    "            bed = prop_soup[0].text.replace(' bd','')\n",
    "            num_bed.append(int(bed))\n",
    "        except:\n",
    "            num_bed.append(None)\n",
    "        time.sleep(np.random.randint(1,2))\n",
    "    #Bath   \n",
    "        try:\n",
    "            bath = prop_soup[1].text.replace(' ba','')\n",
    "            num_bath.append(float(bath))\n",
    "        except:\n",
    "            num_bath.append(None)\n",
    "            \n",
    "        time.sleep(np.random.randint(1,2))\n",
    "    #Sqft\n",
    "        try:\n",
    "            sqft = prop_soup[2].text.replace(' sqft','').replace(',','')\n",
    "            sqft_list.append(int(sqft))\n",
    "        except:\n",
    "            sqft_list.append(None)\n",
    "        time.sleep(np.random.randint(1,2))\n",
    "        \n",
    "        \n",
    "#FACT AND FEATURE LABEL\n",
    "        fact_label_soup=soup.find_all(\"div\", class_=\"ds-expandable-card-section-default-padding\")\n",
    "        fact_label_list=[fact_label_soup[i].text for i in range(len(fact_label_soup))]\n",
    "        fact_soup=soup.find_all(\"span\", class_=\"ds-home-fact-list-item\")\n",
    "#House Type\n",
    "        try:\n",
    "            if 'Type:' in fact_label_list:\n",
    "                Type.append(fact_soup[fact_label_list.index('Type:')].text)\n",
    "            else:\n",
    "                Type.append(None)\n",
    "        except:\n",
    "            Type.append(None)\n",
    "        time.sleep(np.random.randint(1,2))\n",
    "#Year built\n",
    "        try:\n",
    "            if 'Year built:' in fact_label_list:\n",
    "                Year.append(fact_soup[fact_label_list.index('Year built:')].text)\n",
    "            else:\n",
    "                Year.append(None)\n",
    "        except:\n",
    "            Year.append(None)\n",
    "        time.sleep(np.random.randint(1,3))\n",
    "#Heating\n",
    "        try:\n",
    "            if 'Heating:' in fact_label_list:\n",
    "                Heating.append(fact_soup[fact_label_list.index('Heating:')].text)\n",
    "            else:\n",
    "                Heating.append(None)\n",
    "        except:\n",
    "            Heating.append(None)\n",
    "        time.sleep(np.random.randint(1,2))\n",
    "#Cooling\n",
    "        try:\n",
    "            if 'Cooling:' in fact_label_list:\n",
    "                Cooling.append(fact_soup[fact_label_list.index('Cooling:')].text)\n",
    "            else:\n",
    "                Cooling.append(None)\n",
    "        except:\n",
    "            Cooling.append(None)\n",
    "        time.sleep(np.random.randint(1,3))\n",
    "#Parking\n",
    "        try:\n",
    "            if 'Parking:' in fact_label_list:\n",
    "                text=fact_soup[fact_label_list.index('Parking:')].text.lower()\n",
    "                if text.startswith('no data')==True:\n",
    "                    Parking.append(None)\n",
    "                    Space.append(None)\n",
    "                elif 'space' in text:\n",
    "                    Parking.append('Space')\n",
    "                    Space.append(int(re.findall('(.+?) spaces?',text)[0]))\n",
    "                elif text.startswith('attached garage')==True:\n",
    "                    Parking.append(\"Attached Garage\")\n",
    "                    Space.append('NA')\n",
    "                elif text.startswith('carport')==True:\n",
    "                    Parking.append('Carport')\n",
    "                    Space.append('NA')\n",
    "                else:\n",
    "                    Parking.append(text)\n",
    "                    Space.append('NA')\n",
    "            else:\n",
    "                Parking.append('No Parking')\n",
    "                Space.append(0)\n",
    "        except:\n",
    "            Parking.append(None)\n",
    "            Space.append(None)\n",
    "        time.sleep(np.random.randint(1,2))\n",
    "#HOA\n",
    "        try:\n",
    "            if 'HOA:' in fact_label_list:\n",
    "                text=fact_soup[fact_label_list.index('HOA:')].text.lower()\n",
    "                if '/month' in text:\n",
    "                    HOA.append(int(re.findall('\\$(.+?)/month',text)[0].replace(',','')))\n",
    "                elif 'none' in text:\n",
    "                    HOA.append(0)\n",
    "                else:\n",
    "                    HOA.append(text)\n",
    "            else:\n",
    "                HOA.append(None)\n",
    "        except:\n",
    "            HOA.append(None)\n",
    "        time.sleep(np.random.randint(1,3))\n",
    "#Price/sqft\n",
    "        try:\n",
    "            if 'Price/sqft:' in fact_label_list:\n",
    "                text=fact_soup[fact_label_list.index('Price/sqft:')].text.lower()\n",
    "                Price_per_sqft.append(int(text.replace('$','').replace(',','')))\n",
    "            else:\n",
    "                Price_per_sqft.append(None)\n",
    "        except:\n",
    "            Price_per_sqft.append(None)\n",
    "        time.sleep(np.random.randint(1,2))\n",
    "#Lot\n",
    "        try:\n",
    "            if 'Lot:' in fact_label_list:\n",
    "                text=fact_soup[fact_label_list.index('Lot:')].text.lower()\n",
    "                if 'acres' in text:\n",
    "                    Lot.append(round(float(re.findall('(.*?) acres',text)[0])*43560,0))\n",
    "                elif 'sqft' in text:\n",
    "                    sqft=text.replace(',','').replace(' sqft','')\n",
    "                    Lot.append(int(sqft))\n",
    "                else:\n",
    "                    Lot.append(text)\n",
    "            else:\n",
    "                Lot.append(None)\n",
    "        except:\n",
    "            Lot.append(None)\n",
    "        time.sleep(np.random.randint(1,2))\n",
    "            \n",
    "#Days On Zillow\n",
    "        days = soup.find('div', class_=\"sc-qamJO fiTUCr\")\n",
    "        try:\n",
    "            if \"days\" in days.text:\n",
    "                daysOnZillow.append(days.text)\n",
    "            elif 'hours' in days.text:\n",
    "                daysOnZillow.append(days.text)\n",
    "            else:\n",
    "                daysOnZillow.append(None)\n",
    "        except:\n",
    "            daysOnZillow.append(None)\n",
    "        time.sleep(np.random.randint(1,3))\n",
    "#URL\n",
    "        photoCard = soup.find('div', class_= \"ds-data-col ds-white-bg ds-data-col-data-forward\")\n",
    "        link = photoCard.find('script', {'type': 'application/ld+json'})\n",
    "        script_json = json.loads(link.contents[0])\n",
    "        urlTail = str(script_json['url'])\n",
    "        fullLink = 'https://www.zillow.com'+urlTail\n",
    "        UrlList.append(fullLink)\n",
    "        time.sleep(np.random.randint(1,2))\n",
    "#Listing price\n",
    "        priceSoup = soup.find('h4', class_=\"Text-c11n-8-18-0__aiai24-0 StyledHeading-c11n-8-18-0__ktujwe-0 gcaUyc sc-qZtCU kijwKF\")\n",
    "        try:\n",
    "            if \"$\" in priceSoup.text:\n",
    "                cleanPrice = priceSoup.text.replace('$','').replace(',','')\n",
    "                listPrice.append(int(cleanPrice))\n",
    "            else:\n",
    "                listPrice.append(None)\n",
    "        except:\n",
    "            listPrice.append(None)\n",
    "        time.sleep(np.random.randint(1,2))\n",
    "\n",
    "\n",
    "#Address\n",
    "        address = soup.find('h1', class_='Text-c11n-8-18-0__aiai24-0 StyledHeading-c11n-8-18-0__ktujwe-0 efSAZl')\n",
    "        cleanAddress = address.text.replace('\\xa0','')\n",
    "        splitAddress= cleanAddress.split(',')\n",
    "        street = splitAddress[0]\n",
    "        city = splitAddress[1]\n",
    "        stateZip = splitAddress[2].split(' ')\n",
    "        state = stateZip[1]\n",
    "        zipCode = stateZip[2]\n",
    "        \n",
    "        \n",
    "        streetList.append(street)\n",
    "        cityList.append(city)\n",
    "        stateList.append(state)\n",
    "        zipCodeList.append(zipCode)\n",
    "        fullAddress.append(cleanAddress)\n",
    "\n",
    "\n",
    "        time.sleep(np.random.randint(15,20))\n",
    "        driver.delete_all_cookies() \n",
    "        driver.get(str(property_url_list[i+1]))\n",
    "        soup = BS(driver.page_source, 'html.parser')\n",
    "    return status, num_bed, num_bath, sqft_list, listPrice, Type, Year, Heating, Cooling, Parking, Space, HOA, Price_per_sqft, Lot, daysOnZillow, UrlList, streetList, cityList, stateList, zipCodeList, fullAddress\n",
    "\n",
    "\n",
    "def main(csv,start_index, end_index):\n",
    "    url_df=pd.read_csv(csv)\n",
    "    scrape_data=get_property_detail(url_df['url'], driver, start_index, end_index)\n",
    "    df_output=url_df.iloc[start_index:end_index]\n",
    "    df_output[\"Status\"]=scrape_data[0]\n",
    "    df_output[\"Num of bed\"]=scrape_data[1]\n",
    "    df_output[\"Num of bath\"]=scrape_data[2]\n",
    "    df_output[\"Sqft\"]=scrape_data[3]\n",
    "    df_output[\"Listing Price\"]=scrape_data[4]\n",
    "    df_output[\"House Type\"]=scrape_data[5]\n",
    "    df_output[\"Year built\"]=scrape_data[6]\n",
    "    df_output[\"Heating\"]=scrape_data[7]\n",
    "    df_output[\"Cooling\"]=scrape_data[8]\n",
    "    df_output[\"Parking Type\"]=scrape_data[9]\n",
    "    df_output[\"Num of parking space\"]=scrape_data[10]\n",
    "    df_output[\"HOA\"]=scrape_data[11]\n",
    "    df_output[\"Price per sqft\"]=scrape_data[12]\n",
    "    df_output[\"Lot Size in sqft\"]=scrape_data[13]\n",
    "    df_output[\"Days On Zillow\"]=scrape_data[14]\n",
    "    df_output[\"Property Url\"]=scrape_data[15]\n",
    "    df_output[\"Street\"]=scrape_data[16]\n",
    "    df_output[\"City\"]=scrape_data[17]\n",
    "    df_output[\"State\"]=scrape_data[18]\n",
    "    df_output[\"Zip Code\"]=scrape_data[19]\n",
    "    df_output[\"Full Address\"]=scrape_data[20]\n",
    "    return df_output\n",
    "\n",
    "#Will index through by 10\n",
    "ForSaleDF = main('/Users/truon/Documents/BAN612/Zillow/GitTestRun/ZillowForSaleURLFeb12.csv',0,5)\n",
    "ForSaleDF.to_csv('/Users/truon/Documents/BAN612/Zillow/GitTestRun/SJFeb12_sale.csv', index=False)\n",
    "#ForSaleDF = main('/Users/truon/Documents/BAN612/Zillow/ZillowSoldURLSanMateo.csv',0,10)\n",
    "#ForSaleDF.to_csv('/Users/truon/Documents/BAN612/Zillow/SanMateoForSale.csv', index=False) #index=False, mode='a', header=False) to append into csv\n",
    "#Scroll down for the header\n",
    "ForSaleDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_df=pd.DataFrame(columns=[\"Url\",\"Address\",\"Status\",\"Bed\",\"Bath\",\"Price\",\"Year built\",\"Days on zillow\",\"Lot size(sqft)\",\"House size(sqft)\",\"Heating\",\"Cooling\",\"Parking type\",\"Parking space\",\"Price/sqft\",\"HOA\"])\n",
    "City=['SanJose','Richmond','SanLeandro','SanMateo']\n",
    "\n",
    "#For Sale\n",
    "for i in City:\n",
    "    df=pd.DataFrame()\n",
    "    print('processing',i+\"'s sale data\")\n",
    "    file_h=pd.read_csv((i+'_sale.csv'))\n",
    "    df['Url']=file_h['Url']\n",
    "    df['Address']=file_h['Address']\n",
    "    df.insert(1, \"City\", i,  allow_duplicates=True)\n",
    "    df['Status']='Sale'\n",
    "    df['Bed']=[str(i).replace(' bd','') for i in file_h['Bed']]\n",
    "    df['Bath']=[str(i).replace(' ba','') for i in file_h['Bath']]\n",
    "    Price_list=[]\n",
    "    index=0\n",
    "    for i in file_h[\"Price\"]:\n",
    "        i=str(i)\n",
    "        if i=='None' or i==None:\n",
    "            print('row',str(index),'is missing Price')\n",
    "            Price_list.append(None)\n",
    "        else:\n",
    "            if 'm' in i.lower():\n",
    "                new_p=float(i.lower().replace('m','').replace('$',''))*1000000\n",
    "                Price_list.append(new_p)\n",
    "            else:\n",
    "                Price_list.append(float(i.replace(',','').replace('$','')))\n",
    "        #print(f'price index:%d' %index)\n",
    "        index+=1\n",
    "    df['Price']=Price_list\n",
    "    Year_list=[i if (i !='None') or (i !=None) else None for i in file_h['Year built']]\n",
    "    df['Year built']=Year_list\n",
    "    Days_zillow=[]\n",
    "    for i in file_h['Days on zillow']:\n",
    "        i=str(i)\n",
    "        if i=='None' or i==None:\n",
    "            day=None\n",
    "        elif 'day' in i.lower():\n",
    "            day=float(i.lower().replace(' day','').replace('s',''))\n",
    "        elif 'hour' in i.lower():\n",
    "            day=float(i.lower().replace(' hour','').replace('s',''))/24\n",
    "        else:\n",
    "            day=float(i)\n",
    "        Days_zillow.append(day)\n",
    "    df['Days on zillow']=Days_zillow\n",
    "    Lot_size=[]\n",
    "    for i in file_h['Lot size(sqft)']:\n",
    "        i=str(i)\n",
    "        if i=='None' or i==None or i.lower()=='no data':\n",
    "            lot_sqft=None\n",
    "        elif 'acres' in i.lower():\n",
    "            lot_sqft=round(float(re.findall('(.*?) acres',i.lower())[0])*43560,0)\n",
    "        elif 'sqft' in i.lower():\n",
    "            lot_sqft=float(i.lower().replace(',','').replace(' sqft',''))\n",
    "        else:\n",
    "            lot_sqft=float(i)\n",
    "        Lot_size.append(lot_sqft)\n",
    "    df['Lot size(sqft)']=Lot_size\n",
    "    House_size=[]\n",
    "    for i in file_h[\"House size(sqft)\"]:\n",
    "        i=str(i)\n",
    "        if i=='None' or i==None or i.lower()=='no data':\n",
    "            house=None\n",
    "        elif 'sqft' in i.lower():\n",
    "            house=float(i.lower().replace(',','').replace(' sqft',''))\n",
    "        elif 'square feet' in i.lower():\n",
    "            house=float(i.lower().replace(',','').replace(' square feet',''))\n",
    "        else:\n",
    "            house=float(i)\n",
    "        House_size.append(house)\n",
    "    df[\"House size(sqft)\"]=House_size\n",
    "    #heat_list=[]\n",
    "    #for i in file_h[\"Heating\"]:\n",
    "     #   if i=None or i.lower()='no data':\n",
    "      #      heat=None\n",
    "       # else:\n",
    "        #    heat=i\n",
    "        #heat_list.append(heat)\n",
    "    df[\"Heating\"]=[None if (i==None) or (str(i).lower()=='no data') else str(i) for i in file_h[\"Heating\"]]\n",
    "    df[\"Cooling\"]=[None if (i==None) or (str(i).lower()=='no data') else str(i) for i in file_h[\"Cooling\"]]\n",
    "    Parking_type=[]\n",
    "    Space_num=[]\n",
    "    if 'Parking' in file_h.columns:\n",
    "        for i in file_h['Parking']:\n",
    "            i=str(i)\n",
    "            if i==None:\n",
    "                Parking_type.append(None)\n",
    "                Space_num.append(None)\n",
    "            else:\n",
    "                text=i.lower()\n",
    "                if 'no data' in text:\n",
    "                    Parking_type.append(None)\n",
    "                    Space_num.append(None)\n",
    "                elif 'space' in text:\n",
    "                    Parking_type.append('Space')\n",
    "                    Space_num.append(int(re.findall('(.+?) spaces?',text)[0]))\n",
    "                elif 'garage' in text:\n",
    "                    Parking_type.append(\"Garage\")\n",
    "                    Space_num.append('NA')\n",
    "                elif 'carport' in text:\n",
    "                    Parking_type.append('Carport')\n",
    "                    Space_num.append('NA')    \n",
    "                elif 'off street' in text:\n",
    "                    Parking_type.append('Off Street')\n",
    "                    Space_num.append('NA') \n",
    "                else:\n",
    "                    Parking_type.append('No Parking')\n",
    "                    Space_num.append(0)\n",
    "        df[\"Parking type\"]=Parking_type\n",
    "        df[\"Parking space\"]=Space_num\n",
    "    else:\n",
    "        df[\"Parking space\"]=file_h[\"Parking space\"]\n",
    "        for i in file_h['Parking type']:\n",
    "            i=str(i)\n",
    "            if i==None:\n",
    "                Parking_type.append(None)\n",
    "            else:\n",
    "                text=i.lower()\n",
    "                if 'no data' in text:\n",
    "                    Parking_type.append(None)\n",
    "                elif 'space' in text:\n",
    "                    Parking_type.append('Space')\n",
    "                elif 'garage' in text:\n",
    "                    Parking_type.append(\"Garage\")\n",
    "                elif 'carport' in text:\n",
    "                    Parking_type.append('Carport')\n",
    "                elif 'off street' in text:\n",
    "                    Parking_type.append('Off Street')\n",
    "                else:\n",
    "                    Parking_type.append('No Parking')\n",
    "        df[\"Parking type\"]=file_h[\"Parking type\"]\n",
    "    price_sqft=[None if (i==None) or (str(i).lower()=='none') or (str(i).lower()=='no data') else float(str(i).replace('$','').replace(',','')) for i in file_h[\"Price/sqft\"]]\n",
    "    df[\"Price/sqft\"]=price_sqft\n",
    "    df['HOA']=[None if (str(i)=='nan') or (i==None) else 0 if str(i).lower()=='none' else str(i).lower().replace('$','').replace('/month','') for i in file_h[\"HOA\"]]\n",
    "    zillow_df=zillow_df.append(df)\n",
    "zillow_df.insert(15, \"Sold date\", \"99/99/9999\") \n",
    "zillow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv3=pd.read_csv(\"dfv3_test.csv\")\n",
    "dfv3=dfv3.drop(columns=['Unnamed: 0'])\n",
    "#filter out Price, Bed and Bath na\n",
    "dfv3=dfv3[dfv3[\"Bed\"].isna()==False]\n",
    "dfv3=dfv3[dfv3[\"Bath\"].isna()==False]\n",
    "dfv3=dfv3[dfv3[\"Price\"].isna()==False]\n",
    "dfv3.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filled missing value for Years with the mode based on the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfv3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-5a2cc3281ec8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdfv3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdfv3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"City\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'SanJose'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Year built'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdfv3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Year built'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdfv3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Year built'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfv3' is not defined"
     ]
    }
   ],
   "source": [
    "mode=dfv3[dfv3[\"City\"]=='SanJose']['Year built'].mode()[0]\n",
    "print(mode)\n",
    "dfv3['Year built'].fillna(mode,inplace = True)\n",
    "dfv3['Year built'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filled in missing value of lot size with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lot size(sqft):median\n",
    "median=dfv3['Lot size(sqft)'].median()\n",
    "print(median)\n",
    "dfv3['Lot size(sqft)'].fillna(median,inplace = True)\n",
    "dfv3['Lot size(sqft)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filled in missing value for Heating with mode and then transform into a Boolean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
